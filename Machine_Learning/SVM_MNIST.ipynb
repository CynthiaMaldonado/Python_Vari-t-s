{"cells":[{"cell_type":"markdown","metadata":{"id":"dxxRKC9Gpcnn","colab_type":"text"},"source":["# Train an SVM classifier on the MNIST dataset. \n","\n","Since SVM classifiers are binary classifiers, you will need to use one-versus-all to classify all 10 digits. You may want to tune the hyperparameters using small validation sets to speed up the process. What accuracy can you reach?_"]},{"cell_type":"markdown","metadata":{"id":"1cgawSFHpcno","colab_type":"text"},"source":["First, let's load the dataset and split it into a training set and a test set. We could use `train_test_split()` but people usually just take the first 60,000 instances for the training set, and the last 10,000 instances for the test set (this makes it possible to compare your model's performance with others): \n","Don't forget to convert target to int."]},{"cell_type":"code","metadata":{"id":"dy2_qOH0pcnp","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn import svm\n","from sklearn import metrics\n","from sklearn.svm import SVC\n","from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import reciprocal, uniform\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["train_data = pd.read_csv('mnist_train.csv')\n","test_data = pd.read_csv('mnist_test.csv')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n\n   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n0      0      0      0      0      0      0      0      0  \n1      0      0      0      0      0      0      0      0  \n2      0      0      0      0      0      0      0      0  \n3      0      0      0      0      0      0      0      0  \n4      0      0      0      0      0      0      0      0  \n\n[5 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>1x1</th>\n      <th>1x2</th>\n      <th>1x3</th>\n      <th>1x4</th>\n      <th>1x5</th>\n      <th>1x6</th>\n      <th>1x7</th>\n      <th>1x8</th>\n      <th>1x9</th>\n      <th>...</th>\n      <th>28x19</th>\n      <th>28x20</th>\n      <th>28x21</th>\n      <th>28x22</th>\n      <th>28x23</th>\n      <th>28x24</th>\n      <th>28x25</th>\n      <th>28x26</th>\n      <th>28x27</th>\n      <th>28x28</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 785 columns</p>\n</div>"},"metadata":{},"execution_count":3}],"source":["test_data.head()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["X_train = train_data.drop('label', axis=1)\n","y_train = train_data.label.astype('int')\n","X_test = test_data.drop('label', axis=1)\n","y_test = test_data.label.astype('int')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"0       7\n1       2\n2       1\n3       0\n4       4\n       ..\n9995    2\n9996    3\n9997    4\n9998    5\n9999    6\nName: label, Length: 10000, dtype: int32"},"metadata":{},"execution_count":5}],"source":["y_test"]},{"cell_type":"markdown","metadata":{"id":"B0UwmLpspcnu","colab_type":"text"},"source":["Many training algorithms are sensitive to the order of the training instances, so it's generally good practice to shuffle them first. However, the dataset is already shuffled, so we do not need to do it."]},{"cell_type":"markdown","metadata":{"id":"aGmIG3lxpcnv","colab_type":"text"},"source":["Let's start simple, with a linear SVM classifier. It will automatically use the One-vs-All (also called One-vs-the-Rest, OvR) strategy, so there's nothing special we need to do. Use a LinearSVC and initialize it with a randon_state to make results reproducible.\n","\n","**Warning**: this may take a few minutes depending on your hardware."]},{"cell_type":"code","metadata":{"id":"8I8LfloMpcnw","colab_type":"code","colab":{}},"source":["lin_svc = svm.LinearSVC(random_state=42)\n","lin_svc.fit(X_train, y_train)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":"LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n          multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n          verbose=0)"},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"hct3ByDppcn0","colab_type":"text"},"source":["Let's make predictions on the training set and measure the accuracy (we don't want to measure it on the test set yet, since we have not selected and trained the final model yet):"]},{"cell_type":"code","metadata":{"id":"PSez1QA9pcn1","colab_type":"code","colab":{},"tags":[]},"source":["y_pred = lin_svc.predict(X_test)\n","print(\"Accuracy:\",metrics.accuracy_score(y_pred, y_test))\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy: 0.8832\n"}]},{"cell_type":"markdown","metadata":{"id":"XkmPCeU4pcn5","colab_type":"text"},"source":["Okay, Probably you got an accuracy bellow 90%. On MNIST that is pretty bad. This linear model is certainly too simple for MNIST, but perhaps we just needed to scale the data first. Use and StandardScaler to do it:\n","Hint: take into account that you should convert the features to float. "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"(60000, 784)"},"metadata":{},"execution_count":8}],"source":["X_train = X_train.astype(float)\n","X_train.shape"]},{"cell_type":"code","metadata":{"id":"FwGjXqWspcn6","colab_type":"code","colab":{}},"source":["scaler = StandardScaler()\n","scaler.fit(X_train)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":"StandardScaler(copy=True, with_mean=True, with_std=True)"},"metadata":{},"execution_count":11}]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["X_train_scl = scaler.transform(X_train)\n","X_test_scl = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"},"metadata":{},"execution_count":13}],"source":["X_train_scl"]},{"cell_type":"markdown","metadata":{"id":"LuG_z8NApcn-","colab_type":"text"},"source":["Try to train the model again on the scaled features.\n","**Warning**: this may take a few minutes depending on your hardware."]},{"cell_type":"markdown","metadata":{"id":"zvS8AIQepcoE","colab_type":"text"},"source":["Let's make predictions on the training set and measure the accuracy "]},{"cell_type":"code","metadata":{"jupyter":{"source_hidden":true},"id":"mASUldCBpcn_","colab_type":"code","colab":{}},"source":["lin_svc.fit(X_train_scl, y_train)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":"LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n          multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n          verbose=0)"},"metadata":{},"execution_count":14}]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["y_pred_scl = lin_svc.predict(X_test_scl)"]},{"cell_type":"code","execution_count":16,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy: 0.9132\n"}],"source":["print(\"Accuracy:\",metrics.accuracy_score(y_pred_scl, y_test))"]},{"cell_type":"code","metadata":{"id":"C33mRKNJpcoF","colab_type":"code","outputId":"6df38c9a-7bdb-454d-b7ab-319e06617b21","colab":{}},"source":[""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9225333333333333"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"6SWZTf6SpcoM","colab_type":"text"},"source":["That's much better (we cut the error rate by about 25%), but still not great at all for MNIST. If we want to use an SVM, we will have to use a kernel. Let's try an `SVC` with an RBF kernel (the default).\n","#### **Let's train the model only on 10000 samples**  "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"(10000, 784)"},"metadata":{},"execution_count":17}],"source":["X_train_tenthou = X_train.iloc[:10000]\n","y_train_tenthou = y_train.iloc[:10000]\n","X_train_tenthou.shape"]},{"cell_type":"markdown","metadata":{"id":"9GflHUdwpcoN","colab_type":"text"},"source":["**Note**: to be future-proof set `gamma=\"scale\"` since it will be the default value in Scikit-Learn 0.22.\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)"},"metadata":{},"execution_count":18}],"source":["# Create a SVC classifier using a linear kernel\n","svm = SVC(gamma='scale')\n","# Train the classifier\n","svm.fit(X_train_tenthou, y_train_tenthou)"]},{"cell_type":"code","metadata":{"id":"6u1Sch9ApcoO","colab_type":"code","outputId":"4648bf59-75ca-4426-fc77-a88832ba5ab6","colab":{}},"source":[""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n","    max_iter=-1, probability=False, random_state=None, shrinking=True,\n","    tol=0.001, verbose=False)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"XBWE9tMqpcoS","colab_type":"text"},"source":["Let's make predictions on the training set and measure the accuracy "]},{"cell_type":"code","execution_count":20,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy: 0.9594\n"}],"source":["y_pred_RBF = svm.predict(X_test)\n","print(\"Accuracy:\",metrics.accuracy_score(y_pred_RBF, y_test))"]},{"cell_type":"code","metadata":{"id":"uqQaBrGRpcoT","colab_type":"code","outputId":"a54cd356-ac06-4949-ad30-381bc899d45a","colab":{}},"source":[""],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9455333333333333"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"QK4YK3J6pcoX","colab_type":"text"},"source":["That's promising, we get better performance even though we trained the model on 6 times less data. Let's tune the hyperparameters by doing a randomized search with cross validation. We will do this on a small dataset (1000 samples) just to speed up the process:\n","Use a RandomizedSearchCV"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_thou = X_train.iloc[:1000]\n","y_train_thou = y_train.iloc[:1000]"]},{"cell_type":"code","metadata":{"id":"DHGoDbhbpcoX","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import reciprocal, uniform\n","estimator_RBF = SVC(gamma='scale')\n","param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n","\n","CVrandom_RBF = RandomizedSearchCV(estimator=estimator_RBF, param_distributions=param_distributions, random_state=42)\n","search_RBF = CVrandom_RBF.fit(X_train_thou, y_train_thou)\n","search_RBF.best_params_\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qhXos2FOpcoc","colab_type":"text"},"source":["Let's print the best estimator and the best score"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best = search_RBF.best_estimator_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["search_RBF.best_score_"]},{"cell_type":"markdown","metadata":{"id":"yK98k4w-pcok","colab_type":"text"},"source":["This looks pretty low but remember we only trained the model on 1,000 instances. Let's retrain the best estimator on the whole training set (**run this at night, it will take hours**):"]},{"cell_type":"code","metadata":{"id":"b6W2iLbWpcol","colab_type":"code","colab":{}},"source":["# Train the classifier\n","best.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GoeGSbz0pcoo","colab_type":"text"},"source":["Let's make predictions on the training set and measure the accuracy "]},{"cell_type":"code","metadata":{"id":"I_MqgrJ-pcop","colab_type":"code","colab":{}},"source":["y_pred_best = best.predict(X_test)\n","print(\"Accuracy:\",metrics.accuracy_score(y_pred_best, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XL9xFGAKpcot","colab_type":"text"},"source":["Ah, this looks good! Let's select this model. Now we can test it on the test set:"]},{"cell_type":"code","metadata":{"id":"VUoxg53Epcou","colab_type":"code","colab":{}},"source":["y_pred_test = best.predict(X_test)\n","print(\"Accuracy:\",metrics.accuracy_score(y_pred_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KzckR3XPpcoy","colab_type":"text"},"source":["Not too bad, but apparently the model is overfitting slightly. It's tempting to tweak the hyperparameters a bit more (e.g. decreasing `C` and/or `gamma`), but we would run the risk of overfitting the test set. Other people have found that the hyperparameters `C=5` and `gamma=0.005` yield even better performance (over 98% accuracy). By running the randomized search for longer and on a larger part of the training set, you may be able to find this as well."]},{"cell_type":"markdown","metadata":{"id":"BfLWYhMKpcoy","colab_type":"text"},"source":["## Train an SVM regressor on the California housing dataset"]},{"cell_type":"markdown","metadata":{"id":"Azamf_wdpcoz","colab_type":"text"},"source":["Let's load the dataset using Scikit-Learn's `fetch_california_housing()` function:"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn import svm\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import LinearSVR\n","from sklearn.svm import SVR\n","from sklearn.metrics import mean_squared_error\n","from sklearn.svm import SVC"]},{"cell_type":"code","metadata":{"id":"NiKD-wAApco0","colab_type":"code","colab":{},"tags":[]},"source":["from sklearn.datasets import fetch_california_housing \n","cali = fetch_california_housing(data_home=None, download_if_missing=True, return_X_y=False)\n","print(cali.DESCR)"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":".. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 20640\n\n    :Number of Attributes: 8 numeric, predictive attributes and the target\n\n    :Attribute Information:\n        - MedInc        median income in block\n        - HouseAge      median house age in block\n        - AveRooms      average number of rooms\n        - AveBedrms     average number of bedrooms\n        - Population    block population\n        - AveOccup      average house occupancy\n        - Latitude      house block latitude\n        - Longitude     house block longitude\n\n    :Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttp://lib.stat.cmu.edu/datasets/\n\nThe target variable is the median house value for California districts.\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. topic:: References\n\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n      Statistics and Probability Letters, 33 (1997) 291-297\n\n"}]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"{'data': array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n           37.88      , -122.23      ],\n        [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n           37.86      , -122.22      ],\n        [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n           37.85      , -122.24      ],\n        ...,\n        [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n           39.43      , -121.22      ],\n        [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n           39.43      , -121.32      ],\n        [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n           39.37      , -121.24      ]]),\n 'target': array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894]),\n 'feature_names': ['MedInc',\n  'HouseAge',\n  'AveRooms',\n  'AveBedrms',\n  'Population',\n  'AveOccup',\n  'Latitude',\n  'Longitude'],\n 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 20640\\n\\n    :Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n    :Attribute Information:\\n        - MedInc        median income in block\\n        - HouseAge      median house age in block\\n        - AveRooms      average number of rooms\\n        - AveBedrms     average number of bedrooms\\n        - Population    block population\\n        - AveOccup      average house occupancy\\n        - Latitude      house block latitude\\n        - Longitude     house block longitude\\n\\n    :Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttp://lib.stat.cmu.edu/datasets/\\n\\nThe target variable is the median house value for California districts.\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. topic:: References\\n\\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n      Statistics and Probability Letters, 33 (1997) 291-297\\n'}"},"metadata":{},"execution_count":40}],"source":["cali"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n...       ...       ...       ...        ...         ...       ...       ...   \n20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n\n       Longitude  \n0        -122.23  \n1        -122.22  \n2        -122.24  \n3        -122.25  \n4        -122.25  \n...          ...  \n20635    -121.09  \n20636    -121.21  \n20637    -121.22  \n20638    -121.32  \n20639    -121.24  \n\n[20640 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.3252</td>\n      <td>41.0</td>\n      <td>6.984127</td>\n      <td>1.023810</td>\n      <td>322.0</td>\n      <td>2.555556</td>\n      <td>37.88</td>\n      <td>-122.23</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.3014</td>\n      <td>21.0</td>\n      <td>6.238137</td>\n      <td>0.971880</td>\n      <td>2401.0</td>\n      <td>2.109842</td>\n      <td>37.86</td>\n      <td>-122.22</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.2574</td>\n      <td>52.0</td>\n      <td>8.288136</td>\n      <td>1.073446</td>\n      <td>496.0</td>\n      <td>2.802260</td>\n      <td>37.85</td>\n      <td>-122.24</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.6431</td>\n      <td>52.0</td>\n      <td>5.817352</td>\n      <td>1.073059</td>\n      <td>558.0</td>\n      <td>2.547945</td>\n      <td>37.85</td>\n      <td>-122.25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.8462</td>\n      <td>52.0</td>\n      <td>6.281853</td>\n      <td>1.081081</td>\n      <td>565.0</td>\n      <td>2.181467</td>\n      <td>37.85</td>\n      <td>-122.25</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20635</th>\n      <td>1.5603</td>\n      <td>25.0</td>\n      <td>5.045455</td>\n      <td>1.133333</td>\n      <td>845.0</td>\n      <td>2.560606</td>\n      <td>39.48</td>\n      <td>-121.09</td>\n    </tr>\n    <tr>\n      <th>20636</th>\n      <td>2.5568</td>\n      <td>18.0</td>\n      <td>6.114035</td>\n      <td>1.315789</td>\n      <td>356.0</td>\n      <td>3.122807</td>\n      <td>39.49</td>\n      <td>-121.21</td>\n    </tr>\n    <tr>\n      <th>20637</th>\n      <td>1.7000</td>\n      <td>17.0</td>\n      <td>5.205543</td>\n      <td>1.120092</td>\n      <td>1007.0</td>\n      <td>2.325635</td>\n      <td>39.43</td>\n      <td>-121.22</td>\n    </tr>\n    <tr>\n      <th>20638</th>\n      <td>1.8672</td>\n      <td>18.0</td>\n      <td>5.329513</td>\n      <td>1.171920</td>\n      <td>741.0</td>\n      <td>2.123209</td>\n      <td>39.43</td>\n      <td>-121.32</td>\n    </tr>\n    <tr>\n      <th>20639</th>\n      <td>2.3886</td>\n      <td>16.0</td>\n      <td>5.254717</td>\n      <td>1.162264</td>\n      <td>1387.0</td>\n      <td>2.616981</td>\n      <td>39.37</td>\n      <td>-121.24</td>\n    </tr>\n  </tbody>\n</table>\n<p>20640 rows × 8 columns</p>\n</div>"},"metadata":{},"execution_count":41}],"source":["X = pd.DataFrame(cali.data, columns=cali['feature_names'])\n","y = cali.target\n","X"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"'X = cali.data\\ny = cali.target'"},"metadata":{},"execution_count":42}],"source":["#Sin df\n","'''X = cali.data\n","y = cali.target'''"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"Index(['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup',\n       'Latitude', 'Longitude'],\n      dtype='object')"},"metadata":{},"execution_count":43}],"source":["X.columns"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"},"metadata":{},"execution_count":44}],"source":["y"]},{"cell_type":"markdown","metadata":{"id":"g6ZNFTKipco3","colab_type":"text"},"source":["Split it into a training set and a test (20%) set:"]},{"cell_type":"code","metadata":{"id":"MqDXx8Bmpco4","colab_type":"code","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n","type(y_train)"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":"numpy.ndarray"},"metadata":{},"execution_count":45}]},{"cell_type":"code","execution_count":46,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20640 entries, 0 to 20639\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   MedInc      20640 non-null  float64\n 1   HouseAge    20640 non-null  float64\n 2   AveRooms    20640 non-null  float64\n 3   AveBedrms   20640 non-null  float64\n 4   Population  20640 non-null  float64\n 5   AveOccup    20640 non-null  float64\n 6   Latitude    20640 non-null  float64\n 7   Longitude   20640 non-null  float64\ndtypes: float64(8)\nmemory usage: 1.3 MB\n"}],"source":["X.info()"]},{"cell_type":"code","execution_count":47,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 16512 entries, 14196 to 15795\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   MedInc      16512 non-null  float64\n 1   HouseAge    16512 non-null  float64\n 2   AveRooms    16512 non-null  float64\n 3   AveBedrms   16512 non-null  float64\n 4   Population  16512 non-null  float64\n 5   AveOccup    16512 non-null  float64\n 6   Latitude    16512 non-null  float64\n 7   Longitude   16512 non-null  float64\ndtypes: float64(8)\nmemory usage: 1.1 MB\n"}],"source":["X_train.info()"]},{"cell_type":"markdown","metadata":{"id":"xgGlDFo4pco7","colab_type":"text"},"source":["Don't forget to scale the data. Use a StandardScaler"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["scaler = StandardScaler()"]},{"cell_type":"code","metadata":{"id":"q-Vau855pco7","colab_type":"code","colab":{}},"source":["scaler.fit(X_train)\n","X_train_scaled = scaler.transform(X_train)\n","X_test_scaled = scaler.transform(X_test)"],"execution_count":49,"outputs":[]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([[-0.326196  ,  0.34849025, -0.17491646, ...,  0.05137609,\n        -1.3728112 ,  1.27258656],\n       [-0.03584338,  1.61811813, -0.40283542, ..., -0.11736222,\n        -0.87669601,  0.70916212],\n       [ 0.14470145, -1.95271028,  0.08821601, ..., -0.03227969,\n        -0.46014647, -0.44760309],\n       ...,\n       [-0.49697313,  0.58654547, -0.60675918, ...,  0.02030568,\n        -0.75500738,  0.59946887],\n       [ 0.96545045, -1.07984112,  0.40217517, ...,  0.00707608,\n         0.90651045, -1.18553953],\n       [-0.68544764,  1.85617335, -0.85144571, ..., -0.08535429,\n         0.99543676, -1.41489815]])"},"metadata":{},"execution_count":50}],"source":["X_train_scaled"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"numpy.ndarray"},"metadata":{},"execution_count":51}],"source":["type(X_train_scaled)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"array([1.03 , 3.821, 1.726, ..., 2.221, 2.835, 3.25 ])"},"metadata":{},"execution_count":52}],"source":["y_train"]},{"cell_type":"markdown","metadata":{"id":"oyWzTD_Cpco-","colab_type":"text"},"source":["Let's train a simple `LinearSVR` first:"]},{"cell_type":"code","metadata":{"id":"7ry5HJq2pco_","colab_type":"code","colab":{}},"source":["#lin_svc = SVC(kernel ='linear')\n","lin_svr = LinearSVR(random_state=42)\n","lin_svr.fit(X_train_scaled, y_train)\n","y_pred_lin = lin_svr.predict(X_test_scaled)\n","lin_svr"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":"LinearSVR(C=1.0, dual=True, epsilon=0.0, fit_intercept=True,\n          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n          random_state=42, tol=0.0001, verbose=0)"},"metadata":{},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"ZWdkZh3rpcpC","colab_type":"text"},"source":["Let's see how it performs on the training set using the Mean Squared Error:"]},{"cell_type":"code","metadata":{"id":"s34D9Of4pcpD","colab_type":"code","colab":{}},"source":["mean_squared_error(y_test, y_pred_lin)"],"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.5816018911362225"},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"7cX0_vKHpcpG","colab_type":"text"},"source":["Let's look at the RMSE:"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.7626282784792487"},"metadata":{},"execution_count":58}],"source":["mean_squared_error(y_test, y_pred_lin, squared=False)"]},{"cell_type":"code","metadata":{"id":"EzuhB1gNpcpH","colab_type":"code","colab":{},"tags":[]},"source":["from math import sqrt\n","\n","rmse = sqrt(mean_squared_error(y_test, y_pred_lin))\n","\n","print(rmse)"],"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":"0.7626282784792487\n"}]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.5561676539205584"},"metadata":{},"execution_count":60}],"source":["from sklearn.metrics import r2_score\n","r2_score(y_test, y_pred_lin)"]},{"cell_type":"markdown","metadata":{"id":"9GfFW82ApcpK","colab_type":"text"},"source":["In this training set, the targets are tens of thousands of dollars. The RMSE gives a rough idea of the kind of error you should expect (with a higher weight for large errors): so with this model we can expect errors somewhere around $10,000. Not great. Let's see if we can do better with an RBF Kernel. We will use randomized search with cross validation to find the appropriate hyperparameter values for `C` and `gamma`:"]},{"cell_type":"code","metadata":{"id":"eONHPEY7pcpL","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import RandomizedSearchCV\n","from scipy.stats import reciprocal, uniform\n","from scipy.stats import randint\n","estimator_RBF = SVR()\n","param_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\n","\n","CVrandom_RBF = RandomizedSearchCV(estimator=estimator_RBF, param_distributions=param_distributions, random_state=42)\n","search_RBF = CVrandom_RBF.fit(X_train_scaled, y_train)\n","search_RBF.best_params_"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":"{'C': 4.745401188473625, 'gamma': 0.07969454818643928}"},"metadata":{},"execution_count":62}]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"\"from sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.model_selection import RandomizedSearchCV\\n\\nparam_distribs = {\\n        'n_estimators': randint(low=1, high=200),\\n        'max_features': randint(low=1, high=8),\\n    }\\n\\nforest_reg = RandomForestRegressor(random_state=42)\\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\\nrnd_search.fit(X_train_scaled, y_train)\""},"metadata":{},"execution_count":63}],"source":["'''from sklearn.ensemble import RandomForestRegressor\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","param_distribs = {\n","        'n_estimators': randint(low=1, high=200),\n","        'max_features': randint(low=1, high=8),\n","    }\n","\n","forest_reg = RandomForestRegressor(random_state=42)\n","rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n","                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n","rnd_search.fit(X_train_scaled, y_train)'''"]},{"cell_type":"markdown","metadata":{"id":"Y_m1Vg26pcpO","colab_type":"text"},"source":["Let's see the best estimator."]},{"cell_type":"code","metadata":{"id":"EwtoVuKhpcpP","colab_type":"code","colab":{}},"source":["search_RBF.best_estimator_"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":"SVR(C=4.745401188473625, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n    gamma=0.07969454818643928, kernel='rbf', max_iter=-1, shrinking=True,\n    tol=0.001, verbose=False)"},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"PkFGOWnbpcpS","colab_type":"text"},"source":["Now let's measure the RMSE  and the squared error on the training set for this estimator:"]},{"cell_type":"code","metadata":{"id":"NJzgi7i2pcpT","colab_type":"code","colab":{}},"source":["best = search_RBF.best_estimator_\n","best.fit(X_train_scaled, y_train)\n","y_pred_best = best.predict(X_test_scaled)"],"execution_count":65,"outputs":[]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.351561654701232"},"metadata":{},"execution_count":66}],"source":["mean_squared_error(y_test, y_pred_best)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.5929263484626333"},"metadata":{},"execution_count":67}],"source":["mean_squared_error(y_test, y_pred_best, squared=False)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.7317160821248567"},"metadata":{},"execution_count":69}],"source":["from sklearn.metrics import r2_score\n","r2_score(y_test, y_pred_best)"]},{"cell_type":"markdown","metadata":{"id":"rHtQaXY5pcpW","colab_type":"text"},"source":["Looks much better than the linear model. Let's select this model and evaluate it on the test set:"]},{"cell_type":"code","metadata":{"id":"kFFFbeuipcpW","colab_type":"code","colab":{}},"source":["'''best.fit(X_test_scaled, y_test)\n","y_pred_test = best.predict(X_test_scaled)\n","mean_squared_error(y_test, y_pred_test)'''"],"execution_count":139,"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.3112217535523372"},"metadata":{},"execution_count":139}]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":"0.7625002890547018"},"metadata":{},"execution_count":142}],"source":["'''from sklearn.metrics import r2_score\n","r2_score(y_test, y_pred_test)'''"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6-final"},"colab":{"name":"6.1-Ejercicio-Clasification-MNIST-SVM.ipynb","provenance":[{"file_id":"1VLQxQoZl0QS-ntraPtehoVEMXg89hD8_","timestamp":1591602809116}]}},"nbformat":4,"nbformat_minor":0}